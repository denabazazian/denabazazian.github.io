<!DOCTYPE html>
<html lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="google-site-verification" content="FeA8wZzKiI6DkLLqCJMlBhKi6PsRnR1Dmdc2TDgTYcI">
        <title>Dena Bazazian</title>

        <link href="css_js_files/bootstrap.css" rel="stylesheet">
        <link rel="stylesheet" href="css_js_files/academicons.css">
        <link rel="stylesheet" href="css_js_files/font-awesome.css">
	<script async="" src="css_js_files/analytics.js"></script><script async="" defer="defer" src="css_js_files/buttons.js"></script>
    <style id="holderjs-style" type="text/css"></style></head>

    <body>

      <!-- Navigation bar -->
      <div class="navbar navbar-default  navbar-fixed-top bg-info">
        <div class="container">
          <div class="navbar-header">
          
            <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
          </div>
          <div class="navbar-collapse collapse" id="navbar-main">
             
            <ul class="nav navbar-nav navbar-left">
             <!--  <li><a href="#news">News</a></li>  -->
              <!-- <li><a href="#research">Research</a></li>  -->
              <!-- <li><a href="#research">Code</a></li>  -->
            </ul>
          </div>
        </div>
      </div> 

      <!-- end of navigation bar -->

      <div style="height:80px;"></div>
      <div style="height:40px;"></div>

      <!-- CONTENTS -->
      <div class="container">
        <!-- Aboutme -->
        <!--<div id="aboutme"></div> -->
        <div class="row">
          <div class="col-xs-6 col-sm-4 col-md-2">
            <a class="thumbnail">
              <img src="img/Dena.jpeg" alt="Dena Bazazian" class="img-rounded">
            </a>            
          </div>

          <div class="col-xs-10 col-sm-6 col-md-4">
            <h1 class="text-info">Dena Bazazian</h1>
            <h4 class="text-info">Research Scientist, CTTC</h4>
            <h5>
             <!-- <a href="mailto:xxx@xxx.com" class="text-info" title="e-Mail"><i class="fa fa-envelope-square fa-2x"></i></a> -->
	      <!-- <a href="https://scholar.google.com/xxx" class="text-info" title="Google Scholar"><i class="ai ai-google-scholar-square ai-2x"></i></a> -->
             <!-- <a href="https://github.com/xxx" class="text-info" title="GitHub"><i class="fa fa-github-square fa-2x"></i></a> -->
              <!-- <a href="https://www.linkedin.com/xxx/" class="text-info" title="LinkedIn"><i class="fa fa-linkedin-square fa-2x"></i></a> -->
             <!-- <a href="https://twitter.com/xxx" class="text-info" title="Twitter"><i class="fa fa-twitter-square fa-2x"></i></a> -->
            </h5>
          </div>
        </div> <!-- end of Aboutme -->

         <p align="justify">
	<p> Dena Bazazian is a research scientist at CTTC (Centre Tecnològic de Telecomunicacions de Catalunya), her research focuses on computer vision and geometric deep learning algorithms to analyze 3D point clouds. 
Before that she was a postdoctoral researcher at the Computer Vision Center (CVC), Autonomous University of Barcelona (UAB) where she accomplished her PhD in 2018. <br/>

She made research stays at NAVER LABS Europe in Grenoble, France and at Media Integration and Communication Center (MICC), University of Florence, Italy. She was also working on edge extraction and feature description of unorganized point clouds at Universitat Politecnica de Catalunya (UPC) between 2013 and 2015. <br/>

<p> Dena Bazazian was one of the members of the organizing committee of Women in Computer Vision (WiCV) Workshops at CVPR2018 and ECCV2018. Furthermore, she was one of the co-organizers of ICDAR 2017 Robust Reading Challenge on Omnidirectional Video. <br/>

            </p>
        <hr>

        <!-- News -->
       
        <!-- <div class="row" id="news" style="padding-top:60px; margin-top:-60px;">
          <div class="col-md-12">
            <h2>News</h2>
          </div>
        </div>  -->

        <!-- news_1  -->
        <!-- <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">month / year</span>
          </div>
          <div class="col-sm-11 col-md-11">
            NEWS_1_XXXXXXXXXXXXXXXX
          </div>
        </div>
        <div style="height:3px;"></div>

        <div style="height:3px;"></div> -->
         <!-- end_news_1  -->
         <!-- news_2  -->
        <!-- <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-primary">month / year</span>
          </div>
          <div class="col-sm-11 col-md-11">
            NEWS_2_XXXXXXXXXXXXXXXX
          </div>
        </div>
        <div style="height:3px;"></div>  -->
        <!-- end_news_2  -->

        <!-- news_3  -->
        <!-- <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-primary">month / year</span>
          </div>
          <div class="col-sm-11 col-md-11">
		 NEWS_3_XXXXXXXXXXXXXXXX
          </div>
        </div>
        <div style="height:3px;"></div>  -->

	<!-- end_news_3  -->

        <!-- end of news -->




        <hr>
        <!-- Research -->
        <div class="row" id="research" style="padding-top:60px; margin-top:-60px;">
          <div class="col-md-12">
            <h2> <font color=3385c4> Research </font> </h2>
	    <!-- 3c8fcf
            <p>
              See <a href="https://scholar.google.com/xxx">Google Scholar profile</a> for a full list of publications.
            </p>
	    -->
<hr>
<hr>

<!-- textSegmentation -->
            <div class="row">
              <div class="col-xs-10 col-sm-4 col-md-4">
                <a class="thumbnail">
                  <img src="img/textSeg.png" alt="Can Generative Adversarial Networks Teach Themselves Text Segmentation?" width=350>
                </a>
              </div>
              <div class="col-xs-12 col-sm-8 col-md-8">
                <strong>Can Generative Adversarial Networks Teach Themselves Text Segmentation?</strong><br>
		Al-Rawi, M., <u>Bazazian, D.</u>,  Valveny, E.<br>
                <em>ICCV w, </em> 2019.<br>
		<!--<a href=""> <button type="button" class="btn btn-primary btn-xs"> pdf </button> </a> -->
		<a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/AIM/Al-Rawi_Can_Generative_Adversarial_Networks_Teach_Themselves_Text_Segmentation_ICCVW_2019_paper.pdf"><button type="button" class="btn btn-primary btn-xs">PDF</button></a>

          <a href="https://github.com/denabazazian/scene_text_segmentation"><button type="button" class="btn btn-primary btn-xs">Code</button></a>

                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract8">abstract</button>
                <div id="abstract8" class="collapse">
		  <div>
In the information age in which we live, text segmentationfrom scene images is a vital prerequisite task used in manytext understanding applications. Text segmentation is a dif-ficult problem because of the potentially vast variation intext and scene landscape.  Moreover, systems that learn toperform text segmentation usually need non-trivial annota-tion efforts.  We present in this work a novel unsupervisedmethod to segment text at the pixel-level from scene images.The model we propose, which relies on generative adversar-ial neural networks, segments text intelligently;  and doesnot therefore need to associate the scene image that con-tains the text to the ground-truth of the text.  The main ad-vantage is thus skipping the need to obtain the pixel-levelannotation dataset, which is normally required in trainingpowerful text segmentation models. The results are promis-ing, and to the best of our knowledge, constitute the firststep towards reliable unsupervised text segmentation.  Ourwork opens a new research path in unsupervised text seg-mentation and poses many research questions with a lot oftrends available for further improvement.
                  </div>
                </div>
              </div>
            </div>
<!-- end of textSegmentation -->


<hr>
<!-- wordspotting -->
            <div class="row">
              <div class="col-xs-10 col-sm-4 col-md-4">
                <a class="thumbnail">
                  <img src="img/hm_tp_classifier.jpg" alt="Word Spotting in Scene Images based on Character Recognition">
                </a>
              </div>
              <div class="col-xs-12 col-sm-8 col-md-8">
                <strong>Word Spotting in Scene Images based on Character Recognition</strong><br>
		<u>Bazazian, D.</u>,  Karatzas, D. and Bagdanov, A.<br>
                <em>CVPR w, </em> 2018.<br>
		<!--<a href=""> <button type="button" class="btn btn-primary btn-xs"> pdf </button> </a> -->
		<a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w36/Bazazian_Word_Spotting_in_CVPR_2018_paper.pdf"><button type="button" class="btn btn-primary btn-xs">PDF</button></a>

          <a href="https://github.com/denabazazian/wordSpotting_characterRecognition"><button type="button" class="btn btn-primary btn-xs">Code</button></a>

                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract7">abstract</button>
                <div id="abstract7" class="collapse">
		  <div>
In this work we address the challenge of spotting text in scene images without restricting the words to a fixed lexicon or dictionary. Words which are typically out of dictionary include, for instance, cases where exclamation or other punctuation marks are present in words, telephone numbers, URLs, dates, etc.
To this end, we train a Fully Convolutional Network to produce heatmaps of all the character classes. Then, we employ the Text Proposals approach and, via a rectangle classifier, detect the most likely rectangle for each query word based on the character attribute maps. The key advantage of the proposed method is that it allows unconstrained out-of-dictionary word spotting independent from any dictionary or lexicon. We evaluate the proposed method on ICDAR2015 and show that it is capable of identifying and recognizing query words in natural scene images.
                  </div>
                </div>
              </div>
            </div>
<!-- end of wordspotting -->



<hr>
<!-- timeSuppression -->
            <div class="row">
              <div class="col-xs-10 col-sm-4 col-md-4">
                <a class="thumbnail">
                  <img src="img/timeSuppression.jpg" alt="FAST: Facilitated and Accurate Scene TextProposal through FCN Guided Pruning">
                </a>
              </div>
              <div class="col-xs-12 col-sm-8 col-md-8">
                <strong>FAST: Facilitated and Accurate Scene TextProposal through FCN Guided Pruning</strong><br>
		<u>Bazazian, D.</u>,  Gomez, R., Gomez, L., Nicolaou, A., Karatzas, D., and Bagdanov, A.<br>
                <em>Pattern Recognition​ ​ Letter​ ​ (PRL)​ ​ Journal, </em> 2017.<br>
		<!--<a href=""> <button type="button" class="btn btn-primary btn-xs"> pdf </button> </a> -->
		<a href="http://www.sciencedirect.com/science/article/pii/S0167865517302982"><button type="button" class="btn btn-primary btn-xs">PDF</button></a>

          <a href="https://github.com/denabazazian/FAST_TextProposal"><button type="button" class="btn btn-primary btn-xs">Code</button></a>

                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract6">abstract</button>
                <div id="abstract6" class="collapse">
		  <div>
Class-specific text proposal algorithms can efficiently reduce the search space for possible text object locations in an image. In this paper we combine the Text Proposals algorithm with Fully Convolutional Networks to efficiently reduce the number of proposals while maintaining the same recall level and thus gaining a significant speed up. Our experiments demonstrate that such text proposal approaches yield significantly higher recall rates than state-of-the-art text localization techniques, while also producing better-quality localizations. Our results on the ICDAR 2015 Robust Reading Competition (Challenge 4) and the COCO-text datasets show that, when combined with strong word classifiers, this recall margin leads to state-of-the-art results in end-to-end scene text recognition.
                  </div>
                </div>
              </div>
            </div>
<!-- end of timeSuppression -->

<hr>
<!-- compressed -->
            <div class="row">
              <div class="col-xs-10 col-sm-4 col-md-4">
                <a class="thumbnail">
                  <img src="img/crops.png" alt="Reading Text in the Wild from Compressed Images">
                </a>
              </div>
              <div class="col-xs-12 col-sm-8 col-md-8">
                <strong>Reading Text in the Wild from Compressed Images</strong><br>
		Galteri, L., <u>Bazazian, D.</u>, Seidenari, L., Bertini, M., Bagdanov, A., Nicolaou, A., Karatzas, D.<br>
                <em>ICCV w, </em> 2017.<br>
		<!--<a href=""> <button type="button" class="btn btn-primary btn-xs"> pdf </button> </a> -->
		<a href="http://openaccess.thecvf.com/content_ICCV

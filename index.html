<!DOCTYPE html>
<html lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="google-site-verification" content="FeA8wZzKiI6DkLLqCJMlBhKi6PsRnR1Dmdc2TDgTYcI">
        <title>Dena Bazazian</title>
	<!-- Favicon -->
	<link rel="icon" href="img/favicon.png">
        <!-- <link rel="canonical" href="https://denabazazian.com/" /> -->

        <link href="css_js_files/bootstrap.css" rel="stylesheet">
        <link rel="stylesheet" href="css_js_files/academicons.css">
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
        <link rel="stylesheet" href="css_js_files/font-awesome.css">
	<script async="" src="css_js_files/analytics.js"></script><script async="" defer="defer" src="css_js_files/buttons.js"></script>
    <style id="holderjs-style" type="text/css"></style></head>

    <body>

      <!-- Navigation bar -->
      <div class="navbar navbar-default  navbar-fixed-top bg-info">
        <div class="container">
          <div class="navbar-header">
          
            <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
          </div>
          <div class="navbar-collapse collapse" id="navbar-main">
             
            <ul class="nav navbar-nav navbar-left">
             <!--  <li><a href="#news">News</a></li>  -->
              <!-- <li><a href="#research">Research</a></li>  -->
              <!-- <li><a href="#research">Code</a></li>  -->
            </ul>
          </div>
        </div>
      </div> 

      <!-- end of navigation bar -->

      <div style="height:80px;"></div>
      <div style="height:40px;"></div>

      <!-- CONTENTS -->
      <div class="container">
        <!-- Aboutme -->
        <!--<div id="aboutme"></div> -->
        <div class="row">
          <div class="col-xs-6 col-sm-4 col-md-2">
            <a class="thumbnail">
              <img src="img/Dena.jpeg" alt="Dena Bazazian" class="img-rounded">
            </a>            
          </div>

          <div class="col-xs-10 col-sm-6 col-md-4">
            <h1 class="text-info">Dena Bazazian</h1>
            <h4 class="text-info">Senior Research Associate</h4>
	    <h4 class="text-info">University of Bristol</h4>
	    <h6 class="text-info">Visual Information Laboratory</h6>	  

            <h5>
	
	    <!-- hyperlinks info -->
	     <a href="mailto:dena.bazazian@bristol.ac.uk" class="text-info" title="Email"><i class="fa fa-envelope-square fa-2x"></i></a>
	     <a href="https://scholar.google.es/citations?user=_wds938AAAAJ&hl=en" class="text-info" title="Google Scholar"><i class="ai ai-google-scholar-square ai-2x"></i></a>
             <a href="https://github.com/denabazazian" class="text-info" title="GitHub"><i class="fa fa-github-square fa-2x"></i></a>  
             <a href="https://www.bristol.ac.uk/people/person/Dena-Bazazian-d6eb832a-50d6-4ca9-9371-330c0b4b67a2/" class="text-info" title="University Homepage"><i class="fa fa-globe fa-2x"></i></a>  
	   <!--  <a href="https://scholar.google.es/citations?user=_wds938AAAAJ&hl=en" class="text-info" title="Google Scholar"><i class="ai ai-google-scholar-square ai-2x"></i></a> -->

		    
             <!-- <a href="mailto:xxx@xxx.com" class="text-info" title="e-Mail"><i class="fa fa-envelope-square fa-2x"></i></a> -->
	      <!-- <a href="https://scholar.google.com/xxx" class="text-info" title="Google Scholar"><i class="ai ai-google-scholar-square ai-2x"></i></a> -->
             <!-- <a href="https://github.com/xxx" class="text-info" title="GitHub"><i class="fa fa-github-square fa-2x"></i></a> -->
              <!-- <a href="https://www.linkedin.com/xxx/" class="text-info" title="LinkedIn"><i class="fa fa-linkedin-square fa-2x"></i></a> -->
             <!-- <a href="https://twitter.com/xxx" class="text-info" title="Twitter"><i class="fa fa-twitter-square fa-2x"></i></a> -->
            </h5>
          </div>
        </div> <!-- end of Aboutme -->

         <p align="justify">
	<p> Dena Bazazian is a senior research associate at the Visual Information Laboratory of the University of Bristol. Previously, she was a research scientist at CTTC (Centre Tecnològic de Telecomunicacions de Catalunya) <!-- her research focuses on computer vision and geometric deep learning algorithms to analyze 3D point clouds. -->
<!-- Before that, she was -->
		and a postdoctoral researcher at the Computer Vision Center (CVC), Universitat Autònoma de Barcelona (UAB) where she accomplished her PhD in 2018. <br/>

She had long-term research visits at NAVER LABS Europe in Grenoble, France in 2019 and at the Media Integration and Communication Center (MICC), University of Florence, Italy in 2017. She was working with the Image Processing Group (GPI) at Universitat Politècnica de Catalunya (UPC) between 2013 and 2015.<br/> <!-- on sharp edge extraction and feature description of unorganized point clouds-->		

<p> Dena Bazazian was one of the organizers of Deep Learning for Geometric Computing (DLGC) workshops at ICCV2021, CVPR2020, Women in Computer Vision (WiCV) Workshops at CVPR2018 and ECCV2018, Robust Reading Challenge on Omnidirectional Video at ICDAR2017.<br/>	
            </p>
        <hr>

        <!-- News -->
       
        <!-- <div class="row" id="news" style="padding-top:60px; margin-top:-60px;">
          <div class="col-md-12">
            <h2>News</h2>
          </div>
        </div>  -->

        <!-- news_1  -->
        <!-- <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">month / year</span>
          </div>
          <div class="col-sm-11 col-md-11">
            NEWS_1_XXXXXXXXXXXXXXXX
          </div>
        </div>
        <div style="height:3px;"></div>

        <div style="height:3px;"></div> -->
         <!-- end_news_1  -->
         <!-- news_2  -->
        <!-- <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-primary">month / year</span>
          </div>
          <div class="col-sm-11 col-md-11">
            NEWS_2_XXXXXXXXXXXXXXXX
          </div>
        </div>
        <div style="height:3px;"></div>  -->
        <!-- end_news_2  -->

        <!-- news_3  -->
        <!-- <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-primary">month / year</span>
          </div>
          <div class="col-sm-11 col-md-11">
		 NEWS_3_XXXXXXXXXXXXXXXX
          </div>
        </div>
        <div style="height:3px;"></div>  -->

	<!-- end_news_3  -->

        <!-- end of news -->




        <hr>
        <!-- Research -->
        <div class="row" id="research" style="padding-top:60px; margin-top:-60px;">
          <div class="col-md-12">
            <h2> <font color=3385c4> Research </font> </h2>
	    <!-- 3c8fcf
            <p>
              See <a href="https://scholar.google.com/xxx">Google Scholar profile</a> for a full list of publications.
            </p>
	    -->
<hr>
<hr>


<!-- DDS -->
            <div class="row">
              <div class="col-xs-10 col-sm-4 col-md-4">
                <a class="thumbnail">
                  <img src="img/DDS_main_img.png" alt="Dual-Domain Image Synthesis using Segmentation-Guided GAN"> <!-- width="250" height="100" -->
                </a>
              </div>
              <div class="col-xs-12 col-sm-8 col-md-8">
                <strong> Dual-Domain Image Synthesis using Segmentation-Guided GAN</strong><br>
		<u> Bazazian, D. </u>, Calway, A., Damen, D.<br>
                <em> CVPR w, </em> 2022.<br>
			
		<!--<a href=""> <button type="button" class="btn btn-primary btn-xs"> pdf </button> </a> -->
		<a href="https://arxiv.org/pdf/2204.09015.pdf"><button type="button" class="btn btn-primary btn-xs">PDF</button></a>

                <a href="https://github.com/denabazazian/Dual-Domain-Synthesis"><button type="button" class="btn btn-primary btn-xs">Code</button></a> 
		
	     
	<!--
		<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtext10">BibTex</button>
                <div id="bibtext10" class="collapse">
		  <div>
			@article{Bazazian_DDS2022, <br/>
                        author={Bazazian, Dena and Calway, Andrew and Damen, Dima},<br/>
                        journal={IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, <br/>
                        title={Dual-Domain Image Synthesis using Segmentation-Guided GAN}, <br/>
                        year={2022},<br/>
                        pages={1-16},<br/>
                        doi={https://doi.org/}} <br/>
		       </div>
                </div>
                -->
		      
		      
                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract10">abstract</button>
                <div id="abstract10" class="collapse">
		  <div>
We introduce a segmentation-guided approach to synthesise images that integrate features from two distinct domains. Images synthesised by our dual-domain model belong to one domain within the semantic-mask, and to another in the rest of the image - smoothly integrated. We build on the successes of few-shot StyleGAN and single-shot semantic segmentation to minimise the amount of training required in utilising two domains. The method combines few-shot cross-domain StyleGAN with a latent optimiser to achieve images containing features of two distinct domains. We use a segmentation-guided perceptual loss, which compares both pixel-level and activations between domain-specific and dual-domain synthetic images. Results demonstrate qualitatively and quantitatively that our model is capable of synthesising dual-domain images on a variety of objects (faces, horses, cats, cars), domains (natural, caricature, sketches) and part-based masks (eyes, nose, mouth, hair, car bonnet). The code is publicly available at <a href="https://github.com/denabazazian/Dual-Domain-Synthesis">this URL</a>.		 
			  
                  </div>
                </div>
              </div>
            </div>
<!-- end of DDS -->		  
		  
		  
		  
		  
		  
<!-- EDC-Net -->
            <div class="row">
              <div class="col-xs-10 col-sm-4 col-md-4">
                <a class="thumbnail">
                  <img src="img/EDC_Net_graphical_abstract.jpg" alt="EDC-Net: Edge Detection Capsule Network for 3D Point Clouds"> <!-- width="350" height="400" -->
                </a>
              </div>
              <div class="col-xs-12 col-sm-8 col-md-8">
                <strong> EDC-Net: Edge Detection Capsule Network for 3D Point Clouds</strong><br>
		<u> Bazazian, D. </u>, Parés, ME.<br>
                <em> Applied Sciences Journal, </em> 2021.<br>
		<!--<a href=""> <button type="button" class="btn btn-primary btn-xs"> pdf </button> </a> -->
		<a href="https://www.mdpi.com/2076-3417/11/4/1833"><button type="button" class="btn btn-primary btn-xs">PDF</button></a>

          <!-- <a href="https://github.com/denabazazian/DCG-Net"><button type="button" class="btn btn-primary btn-xs">Code</button></a> -->
		<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtext10">BibTeX</button>
                <div id="bibtext10" class="collapse">
		  <div>
			@article{Bazazian-EDCNet2021, <br/>
                        author={D. {Bazazian} and ME. {Parés}},<br/>
                        journal={Applied Sciences}, <br/>
                        title={EDC-Net: Edge Detection Capsule Network for 3D Point Clouds}, <br/>
                        year={2021},<br/>
                        volume={11},<br/>
                        number={4: 1833},<br/>
                        pages={1-16},<br/>
                        doi={https://doi.org/10.3390/app11041833}} <br/>
		       </div>
                </div>
		      
		      
                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract10">abstract</button>
                <div id="abstract10" class="collapse">
		  <div>
Edge features in point clouds are prominent due to the capability of describing an abstract shape of a set of points. Point clouds obtained by 3D scanner devices are often immense in terms of size. Edges are essential features in large scale point clouds since they are capable of describing the shapes in down-sampled point clouds while maintaining the principal information. In this paper, we tackle challenges of edge detection tasks in 3D point clouds. To this end, we propose a novel technique to detect edges of point clouds based on a capsule network architecture. In this approach, we define the edge detection task of point clouds as a semantic segmentation problem. We built a classifier through the capsules to predict edge and non-edge points in 3D point clouds. We applied a weakly-supervised learning approach in order to improve the performance of our proposed method and built in the capability of testing the technique in wider range of shapes. We provide several quantitative and qualitative experimental results to demonstrate the robustness of our proposed EDC-Net for edge detection in 3D point clouds. We performed a statistical analysis over the ABC and ShapeNet datasets. Our numerical results demonstrate the robust and efficient performance of EDC-Net. 
		 
			  
                  </div>
                </div>
              </div>
            </div>
<!-- end of EDC-Net -->
		  
<hr>		  

<!-- DCG-Net -->
            <div class="row">
              <div class="col-xs-10 col-sm-4 col-md-4">
                <a class="thumbnail">
                  <img src="img/CG_Conv.png" alt="DCG-Net: Dynamic Capsule Graph Convolutional Network for Point Clouds"> <!-- width="350" height="400" -->
                </a>
              </div>
              <div class="col-xs-12 col-sm-8 col-md-8">
                <strong> DCG-Net: Dynamic Capsule Graph Convolutional Network for Point Clouds</strong><br>
		<u> Bazazian, D. </u>, Nahat, D.<br>
                <em>IEEEAccess Journal, </em> 2020.<br>
		<!--<a href=""> <button type="button" class="btn btn-primary btn-xs"> pdf </button> </a> -->
		<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9226416"><button type="button" class="btn btn-primary btn-xs">PDF</button></a>

          <!-- <a href="https://github.com/denabazazian/DCG-Net"><button type="button" class="btn btn-primary btn-xs">Code</button></a> -->
		<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtext9">BibTeX</button>
                <div id="bibtext9" class="collapse">
		  <div>
			@article{Bazazian2020, <br/>
                        author={D. {Bazazian} and D. {Nahata}},<br/>
                        journal={IEEE Access}, <br/>
                        title={DCG-Net: Dynamic Capsule Graph Convolutional Network for Point Clouds}, <br/>
                        year={2020},<br/>
                        volume={8},<br/>
                        number={},<br/>
                        pages={188056-188067},<br/>
                        doi={10.1109/ACCESS.2020.3031812}} <br/>
		       </div>
                </div>
		      
		      
                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract9">abstract</button>
                <div id="abstract9" class="collapse">
		  <div>
This paper introduces DCG-Net (Dynamic Capsule Graph Network) to analyze point clouds for  the  tasks  of  classification  and  segmentation.  DCG-Net  aggregates  point  cloud  features  to  build  and update  the  graphs  based  on  the  dynamic  routing  mechanism  of  capsule  networks  at  each  layer  of  aconvolutional network. The first layer of DGC-Net exploits the geometrical attributes of the point cloudto build a graph by neighborhood aggregation while the deeper layers of the network dynamically updatethe graph based on the feature space of convolutions. We conduct extensive experiments on public datasets, ModelNet40,  ShapeNet-Part.  Our  experimental  results  demonstrate  that  DCG-Net  achieves  state-of-the-art  performance  on  public  datasets, 93.4% accuracy  on  ModelNet40,  and 85.4% instance  mIoU  (mean Intersection over Union) on ShapeNet-Part. 
		 
			  
                  </div>
                </div>
              </div>
            </div>
<!-- end of DCG-Net -->


<hr>
		  
		  
		  
<!-- textSegmentation -->
            <div class="row">
              <div class="col-xs-10 col-sm-4 col-md-4">
                <a class="thumbnail">
                  <img src="img/textSeg.png" alt="Can Generative Adversarial Networks Teach Themselves Text Segmentation?" width=350>
                </a>
              </div>
              <div class="col-xs-12 col-sm-8 col-md-8">
                <strong>Can Generative Adversarial Networks Teach Themselves Text Segmentation?</strong><br>
		Al-Rawi, M., <u>Bazazian, D.</u>,  Valveny, E.<br>
                <em>ICCV w, </em> 2019.<br>
		<!--<a href=""> <button type="button" class="btn btn-primary btn-xs"> pdf </button> </a> -->
		<a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/AIM/Al-Rawi_Can_Generative_Adversarial_Networks_Teach_Themselves_Text_Segmentation_ICCVW_2019_paper.pdf"><button type="button" class="btn btn-primary btn-xs">PDF</button></a>

          <a href="https://github.com/denabazazian/scene_text_segmentation"><button type="button" class="btn btn-primary btn-xs">Code</button></a>

                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract8">abstract</button>
                <div id="abstract8" class="collapse">
		  <div>
In the information age in which we live, text segmentationfrom scene images is a vital prerequisite task used in manytext understanding applications. Text segmentation is a dif-ficult problem because of the potentially vast variation intext and scene landscape.  Moreover, systems that learn toperform text segmentation usually need non-trivial annota-tion efforts.  We present in this work a novel unsupervisedmethod to segment text at the pixel-level from scene images.The model we propose, which relies on generative adversar-ial neural networks, segments text intelligently;  and doesnot therefore need to associate the scene image that con-tains the text to the ground-truth of the text.  The main ad-vantage is thus skipping the need to obtain the pixel-levelannotation dataset, which is normally required in trainingpowerful text segmentation models. The results are promis-ing, and to the best of our knowledge, constitute the firststep towards reliable unsupervised text segmentation.  Ourwork opens a new research path in unsupervised text seg-mentation and poses many research questions with a lot oftrends available for further improvement.
                  </div>
                </div>
              </div>
            </div>
<!-- end of textSegmentation -->


<hr>
<!-- wordspotting -->
            <div class="row">
              <div class="col-xs-10 col-sm-4 col-md-4">
                <a class="thumbnail">
                  <img src="img/hm_tp_classifier.jpg" alt="Word Spotting in Scene Images based on Character Recognition">
                </a>
              </div>
              <div class="col-xs-12 col-sm-8 col-md-8">
                <strong>Word Spotting in Scene Images based on Character Recognition</strong><br>
		<u>Bazazian, D.</u>,  Karatzas, D. and Bagdanov, A.<br>
                <em>CVPR w, </em> 2018.<br>
		<!--<a href=""> <button type="button" class="btn btn-primary btn-xs"> pdf </button> </a> -->
		<a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w36/Bazazian_Word_Spotting_in_CVPR_2018_paper.pdf"><button type="button" class="btn btn-primary btn-xs">PDF</button></a>

          <a href="https://github.com/denabazazian/wordSpotting_characterRecognition"><button type="button" class="btn btn-primary btn-xs">Code</button></a>

                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract7">abstract</button>
                <div id="abstract7" class="collapse">
		  <div>
In this work we address the challenge of spotting text in scene images without restricting the words to a fixed lexicon or dictionary. Words which are typically out of dictionary include, for instance, cases where exclamation or other punctuation marks are present in words, telephone numbers, URLs, dates, etc.
To this end, we train a Fully Convolutional Network to produce heatmaps of all the character classes. Then, we employ the Text Proposals approach and, via a rectangle classifier, detect the most likely rectangle for each query word based on the character attribute maps. The key advantage of the proposed method is that it allows unconstrained out-of-dictionary word spotting independent from any dictionary or lexicon. We evaluate the proposed method on ICDAR2015 and show that it is capable of identifying and recognizing query words in natural scene images.
                  </div>
                </div>
              </div>
            </div>
<!-- end of wordspotting -->



<hr>
<!-- timeSuppression -->
            <div class="row">
              <div class="col-xs-10 col-sm-4 col-md-4">
                <a class="thumbnail">
                  <img src="img/timeSuppression.jpg" alt="FAST: Facilitated and Accurate Scene TextProposal through FCN Guided Pruning">
                </a>
              </div>
              <div class="col-xs-12 col-sm-8 col-md-8">
                <strong>FAST: Facilitated and Accurate Scene TextProposal through FCN Guided Pruning</strong><br>
		<u>Bazazian, D.</u>,  Gomez, R., Gomez, L., Nicolaou, A., Karatzas, D., and Bagdanov, A.<br>
                <em>Pattern Recognition​ ​ Letter​ ​ (PRL)​ ​ Journal, </em> 2017.<br>
		<!--<a href=""> <button type="button" class="btn btn-primary btn-xs"> pdf </button> </a> -->
		<a href="http://www.sciencedirect.com/science/article/pii/S0167865517302982"><button type="button" class="btn btn-primary btn-xs">PDF</button></a>

          <a href="https://github.com/denabazazian/FAST_TextProposal"><button type="button" class="btn btn-primary btn-xs">Code</button></a>

                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract6">abstract</button>
                <div id="abstract6" class="collapse">
		  <div>
Class-specific text proposal algorithms can efficiently reduce the search space for possible text object locations in an image. In this paper we combine the Text Proposals algorithm with Fully Convolutional Networks to efficiently reduce the number of proposals while maintaining the same recall level and thus gaining a significant speed up. Our experiments demonstrate that such text proposal approaches yield significantly higher recall rates than state-of-the-art text localization techniques, while also producing better-quality localizations. Our results on the ICDAR 2015 Robust Reading Competition (Challenge 4) and the COCO-text datasets show that, when combined with strong word classifiers, this recall margin leads to state-of-the-art results in end-to-end scene text recognition.
                  </div>
                </div>
              </div>
            </div>
<!-- end of timeSuppression -->

<hr>
<!-- compressed -->
            <div class="row">
              <div class="col-xs-10 col-sm-4 col-md-4">
                <a class="thumbnail">
                  <img src="img/crops.png" alt="Reading Text in the Wild from Compressed Images">
                </a>
              </div>
              <div class="col-xs-12 col-sm-8 col-md-8">
                <strong>Reading Text in the Wild from Compressed Images</strong><br>
		Galteri, L., <u>Bazazian, D.</u>, Seidenari, L., Bertini, M., Bagdanov, A., Nicolaou, A., Karatzas, D.<br>
                <em>ICCV w, </em> 2017.<br>
		<!--<a href=""> <button type="button" class="btn btn-primary btn-xs"> pdf </button> </a> -->
	<a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w34/degrees_leonardo.galteriunifi.it_dbazaziancvc.uab.es_lorenzo.seidenariunifi.it_ICCV_2017_paper.pdf"><button type="button" class="btn btn-primary btn-xs">PDF</button></a>

          <a href="https://github.com/denabazazian/ReadingText_CroppedWord"><button type="button" class="btn btn-primary btn-xs">Code</button></a>

                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract5">abstract</button>
                <div id="abstract5" class="collapse">
		  <div>
Reading text in the wild is gaining attention in the computer vision community. Images captured in the wild are
almost always compressed to varying degrees, depending on application context, and this compression introduces artifacts that distort image content into the captured images. In this paper we investigate the impact these compression
artifacts have on text localization and recognition in the wild. We also propose a deep Convolutional Neural Network (CNN) that can eliminate text-specific compression artifacts and which leads to an improvement in text recognition.  Experimental results on the ICDAR-Challenge4 dataset demonstrate that compression artifacts have a significant impact on text localization and recognition and that our approach yields an improvement in both – especially at high compression rates.
                  </div>
                </div>
              </div>
            </div>
<!-- end of compressed -->
<hr>

<!-- textproposal_1 -->
            <div class="row">
              <div class="col-xs-10 col-sm-4 col-md-4">
                <a class="thumbnail">
                  <img src="img/fig_tp_hm_61.jpg" alt="Improving Text Proposals for Scene Images with Fully Convolutional Networks">
                </a>
              </div>
              <div class="col-xs-12 col-sm-8 col-md-8">
                <strong>Improving Text Proposals for Scene Images with Fully Convolutional Networks</strong><br>
		<u>Bazazian, D.</u>,  Gomez, R., Gomez, L., Nicolaou, A., Karatzas, D., and Bagdanov, A.<br>
                <em>ICPR, </em> 2016.<br>
		<!--<a href=""> <button type="button" class="btn btn-primary btn-xs"> pdf </button> </a> -->
		<a href="https://arxiv.org/pdf/1702.05089.pdf"><button type="button" class="btn btn-primary btn-xs">PDF</button></a>

          <a href="https://github.com/denabazazian/pixelWise_textDetector"><button type="button" class="btn btn-primary btn-xs">Code</button></a>

                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract4">abstract</button>
                <div id="abstract4" class="collapse">
		  <div>
Text  Proposals  have  emerged  as  a  class-dependent version  of  object  proposals  –  efficient  approaches  to  reduce the  search  space  of  possible  text  object  locations  in  an  image. Combined  with  strong  word  classifiers,  text  proposals  currently yield   top   state   of   the   art   results   in   end-to-end   scene   text
recognition.  In  this  paper  we  propose  an  improvement  over  the original Text Proposals algorithm, combining it with Fully Convolutional  Networks  to  improve  the  ranking  of  proposals. Results  on  the  ICDAR  RRC  and  the  COCO-text  datasets  show superior  performance  over  current  state-of-the-art.
                  </div>
                </div>
              </div>
            </div>
<!-- end of textproposal_1 -->
<hr>

<!-- DOST -->
            <div class="row">
              <div class="col-xs-10 col-sm-4 col-md-4">
                <a class="thumbnail">
                  <img src="img/DOST.png" alt="ICDAR2017 Robust Reading Challenge on Omnidirectional Video">
                </a>
              </div>
              <div class="col-xs-12 col-sm-8 col-md-8">
                <strong>ICDAR2017 Robust Reading Challenge on Omnidirectional Video</strong><br>
		Iwamura, M., Morimoto, N., Tainaka, K., <u>Bazazian, D.</u>, Gomez, L., Karatzas, D.<br>
                <em>ICDAR, </em> 2017.<br>
		<!--<a href=""> <button type="button" class="btn btn-primary btn-xs"> pdf </button> </a> -->
		<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8270167"><button type="button" class="btn btn-primary btn-xs">PDF</button></a>

          <a href="http://rrc.cvc.uab.es/?ch=7&com=introduction"><button type="button" class="btn btn-primary btn-xs">Website</button></a>

                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract3">abstract</button>
                <div id="abstract3" class="collapse">
		  <div>
This challenge focuses on scene text localization and recognition on the Downtown Osaka Scene Text (DOST) dataset. Five tasks will be opened within this Challenge: Text Localisation in Videos, Text Localisation in Still Images, Cropped Word Recognition, End-to-End Recognition in Videos, and End-toEnd Recognition in Still Images. The DOST dataset preserves scene texts observed in the real environment as they were. The dataset contains videos (sequential images) captured in shopping streets in downtown Osaka with an omnidirectional camera. Use of the omnidirectional camera contributes to excluding user’s intention in capturing images. Sequential images contained in the dataset contribute to encouraging developing a new kind of text detection and recognition techniques that utilize temporal information. Another important feature of DOST dataset is that it contains non-Latin text. Since the images were captured in Japan, a lot of Japanese text is contained while it also contains adequate amount of Latin text. Because of these features of the dataset, we can say that the DOST dataset preserved scene texts in the wild.
                  </div>
                </div>
              </div>
            </div>
<!-- end of DOST -->


<hr>
<!-- segmentationBased -->
            <div class="row">
              <div class="col-xs-10 col-sm-4 col-md-4">
                <a class="thumbnail">
                  <img src="img/segmentationBased_edgeExtraction.png" alt="Segmentation-based Multi-Scale Edge Extraction to Measure the Persistence of Features in Unorganized Point Clouds">
                </a>
              </div>
              <div class="col-xs-12 col-sm-8 col-md-8">
                <strong>Segmentation-based Multi-Scale Edge Extraction to Measure the Persistence of Features in Unorganized Point Clouds</strong><br>
		<u>Bazazian, D.</u>, , Casas, J.R., Ruiz-Hidalgo, J.<br>
                <em>VISAPP, </em> 2017.<br>
		<!--<a href=""> <button type="button" class="btn btn-primary btn-xs"> pdf </button> </a> -->
		<a href="http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0006092503170325"><button type="button" class="btn btn-primary btn-xs">PDF</button></a>

          <a href="https://github.com/denabazazian/Edge_Extraction"><button type="button" class="btn btn-primary btn-xs">Code</button></a>

                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract2">abstract</button>
                <div id="abstract2" class="collapse">
		  <div>
Edge extraction has attracted a lot of attention in computer vision.  The accuracy of extracting edges in point clouds can be a significant asset for a variety of engineering scenarios. To address these issues, we propose a segmentation-based multi-scale edge extraction technique. In this approach, different regions of a point cloud are segmented by a global analysis according to the geodesic distance.  Afterwards, a multi-scale operator is defined according to local neighborhoods. Thereupon, by applying this operator at multiple scales of the point cloud, the persistence of features is determined.  We illustrate the proposed method by computing a feature weight that measures the likelihood of a point to be an edge, then detects the edge points based on that value at both global and local scales.  Moreover, we evaluate quantitatively and qualitatively our method.  Experimental results show that the proposed approach achieves a superior accuracy.  Furthermore, we demonstrate the robustness of our approach in noisier real-world datasets.
                  </div>
                </div>
              </div>
            </div>
<!-- end of segmentationBased -->

<hr>
<!-- EdgeDetection -->
            <div class="row">
              <div class="col-xs-10 col-sm-4 col-md-4">
                <a class="thumbnail">
                  <img src="img/RealImages31.png" alt="Fast and Robust Edge Extraction in unorganized Point Clouds">
                </a>
              </div>
              <div class="col-xs-12 col-sm-8 col-md-8">
                <strong>Fast and Robust Edge Extraction in unorganized Point Clouds</strong><br>
		<u>Bazazian, D.</u>, , Casas, J.R., Ruiz-Hidalgo, J.<br>
                <em>DICTA, </em> 2015.<br>
		<!--<a href=""> <button type="button" class="btn btn-primary btn-xs"> pdf </button> </a> -->
		<a href="https://ieeexplore.ieee.org/document/7371262"><button type="button" class="btn btn-primary btn-xs">PDF</button></a>

          <a href="https://github.com/denabazazian/Edge_Extractionn"><button type="button" class="btn btn-primary btn-xs">Code</button></a>

                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract1">abstract</button>
                <div id="abstract1" class="collapse">
		  <div>
Edges   provide   important   visual   information   in scene  surfaces.  The  need  for  fast  and  robust  feature  extraction from 3D data is nowadays fostered by the widespread availability of  cheap  commercial  depth  sensors  and  multi-camera  setups. This   article   investigates   the   challenge   of   detecting   edges   in surfaces  represented  by  unorganized  point  clouds.  Generally, edge  recognition  requires  the  extraction  of  geometric  features
such as normal vectors and curvatures. Since the normals alone do  not  provide  enough  information  about  the  geometry  of  the
cloud,  further  analysis  of  extracted  normals  is  needed  for  edge extraction, such as a clustering method. Edge extraction through these techniques consists of several steps with parameters which depend  on  the  density  and  the  scale  of  the  point  cloud.  In  this paper we propose a fast and precise method to detect sharp edge features  by  analysing  the  eigenvalues  of  the  covariance  matrix that  are  defined  by  each  point’s  k-nearest  neighbors.  Moreover, we evaluate quantitatively, and qualitatively the proposed methods  for  sharp  edge  extraction  using  several  dihedral  angles  and well known examples of unorganized point clouds. Furthermore, we  demonstrate  the  robustness  of  our  approach  in  the  noisier real-world  datasets
                  </div>
                </div>
              </div>
            </div>
<!-- end of EdgeDetection -->


          </div>
        </div> <!-- end of projects -->
      <hr>
      <hr>
<!-- Media 3380ff --> 
<div class="row" id="Media" style="padding-top:60px; margin-top:-60px;">
          <div class="col-md-12">
            <h2> <font color=3385c4> Media </font> </h2>
<li class="citation">
An interview with the International Association for Pattern Recognition ( IAPR ) about the second IAPR Education Committee Research Scholar. [<a class="code" target="_blank" href="https://iapr.org/docs/newsletter-2019-03.pdf">Link</a> - Pages 7 and 8]  
<li class="citation">
 Fully Convolutional Networks for Text Understanding in Scene Images. CVC, 2018.[<a class="code" target="_blank" href="http://www.cvc.uab.es/?page_id=113&id=299">Video</a>]
<li class="citation">
 The introduction of the fifth Women in Computer Vision Workshop (WiCV) at ECCV 2018.[<a class="code" target="_blank" href="https://www.youtube.com/watch?v=Dk26-cRlzKU&list=PL8KOb02GLRdhKRj6Hvqj2jqkoP2VVUpr8">Video</a>]  
<li class="citation">
 WiCV workshop was featured in the Best of ECCV section of Computer Vision News by RSIPVision. [<a class="code" target="_blank" href="https://www.rsipvision.com/ComputerVisionNews-2018October/20/">Link</a> - Pages 20&mdash;23]  
<li class="citation">
 RSIP article about WiCV at CVPR2018. [<a class="code" target="_blank" href="https://www.rsipvision.com/ComputerVisionNews-2018July/46/">Link</a> - Pages 46&mdash;50]  
<!-- end of Media-->


        <!-- Code -->

       <!-- <div class="row" id="code" style="padding-top:60px; margin-top:-60px;">
          <div class="col-md-12">
            <h2>Code</h2>
	    <div class="row" style="padding-top:60px; margin-top:-60px;">
		    <div class="col-xs-10 col-sm-4 col-md-4 card-body card">
		    <a href="https://github.com/xxxx"> manopth </a> 
			<br>
		    <span></span>
			<br>
			Port of <a href="http://xxxx/"> xxx </a> xxxxx
		    </div>
		    <div class="col-xs-10 col-sm-4 col-md-4 card-body card">
		    <a href="https://github.com/xxxx"> xxxx </a> 
			<br>
		    <span></span>
			<br>
			    xxxx <a href="https:xxxx"> xxx </a> xxx.
		    </div>
		    <div class="col-xs-10 col-sm-4 col-md-4 card-body card">
		    <a href="https://github.com/xxxx"> xxx </a> 
			<br>
		    <span></span>
			<br>
                           xxxx
		    </div>
	    </div>
	    <div class="row" style="padding-top:20px; margin-top:0px;">
		    <div class="col-xs-10 col-sm-4 col-md-4 card-body card">
		    <a href="https://github.com/xxx"> xxx </a> 
			<br>
		    <span></span>
			<br>
			    xxx  
		    </div>
	    </div>
	   </div>
	</div>
      </div> -->
<!-- end of Code-->


	    <!--
            <p>
              See <a href="https://scholar.google.com/xxxxx">Google Scholar profile</a> for a full list of publications.
            </p>
	  </div>
	</div>
	    -->



      <hr>

      <div class="container">
        <footer>
          <p align="right"><small>Copyright © Dena Bazazian &nbsp;/&nbsp; Last update: Apr 2022</small></p>
        </footer>
        <div style="height:10px;"></div>
      </div>

      <!-- Bootstrap core JavaScript -->
      <!-- Placed at the end of the document so the pages load faster -->
      <script src="css_js_files/jquery-1.js"></script>
      <script src="css_js_files/bootstrap.js"></script>
      <script src="css_js_files/docs.js"></script>

      <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-75922772-1', 'auto');
        ga('send', 'pageview');

      </script>




</body></html>

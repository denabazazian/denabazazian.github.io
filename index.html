
<html>
<head>
	<meta charset="UTF-8">
	<meta name="description" content"Dena Bazazian- Research Scientist, CTTC ">
        <meta name="keywords" lang="en" content="Dena Bazazian Computer Vision Center Research Scientist CTTC">
	<!--<meta name="description" content"Dena Bazazian- Postdoctoral Researcher, Computer Vision Center - Universitat Autonoma de Barcelona">
        <meta name="keywords" lang="en" content="Dena Bazazian Computer Vision Center Universitat Autonoma Barcelona">//-->
	<meta name="viewport" content="width=device-width, initial-scale=1" />
	<title>Dena Bazazian | Research Scientist, CTTC</title>
        <link rel="stylesheet" href="styles.css">
        <script src="jquery-latest.min.js" type="text/javascript"></script>
</head>

<body class="homepage">
<div id="page" class="hfeedsite">
<div id="masthead" class="site-header" role="banner" style="background: #d0e0b1;"><hr><hr>
<div class="site-whole-title"><span class="site-title"> <p align="center"> <b>Dena Bazazian </b> |</span><span class="site-description"> Research Scientist, CTTC </p> </span></div><hr>
<div id='space' style="background: #ffffff;"><hr></div>
<div id='cssmenu' class='align-center'>
<ul>
 <!--
   <li class='active'><a href='#about'><span>About</span></a></li>
   <li><a href='#research'><span>Research</span></a></li>
   <li><a href='#publications'><span>Publications</span></a></li>
   <li><a href='#code'><span>Code</span></a></li>
   <li class='last' href='others'><a href='#others'><span>Others</span></a></li>
   //-->
</ul>
</div>
</div><!-- #masthead -->


  <a name="About">
  <h1 class="entry-title">About</h1>
  <div class="entry-content">
    <div class="wrapper">
      <div id="one">
<br> <img src="img/Dena.jpeg" width=120> 
       </div>
      <div id="two">
<p> Dena Bazazian is a research scientist at CTTC (Centre Tecnològic de Telecomunicacions de Catalunya), her research focuses on computer vision and geometric deep learning algorithms to analyze 3D point clouds. 
Before that she was a postdoctoral researcher at the Computer Vision Center (CVC), Autonomous University of Barcelona (UAB) where she accomplished her PhD in 2018. <br/>
<!-- Her research focuses on computer vision and deep learning algorithms to improve text understanding techniques in scene images.<br/> -->
<!--Her research focuses on the combination of visual and textual data to mutually improve text extraction and scene understanding.<br/> -->

She made research stays at NAVER LABS Europe in Grenoble, France and at Media Integration and Communication Center (MICC) in University of Florence, Italy. She was also working on edge extraction and feature description of unorganized point clouds at Universitat Politecnica de Catalunya (UPC) between 2013 and 2015. <br/>

<p> Dena Bazazian was one of the members of the organizing committee of Women in Computer Vision (WiCV) Workshops at CVPR2018 and ECCV2018. Furthermore, she was one of the co-organizers of ICDAR 2017 Robust Reading Challenge on Omnidirectional Video. <br/>

<!--<p> She received her BSc degree in Electronic Engineering and her master in Advanced Information Systems. <br/> -->
      </div>
</div>
<br>

  <a name="Selected Projects">
  <h1 class="entry-title">Selected Projects</h1>
  <div class="entry-content">


    <div class="wrapper">
      <div id="one">
      <img src="img/hm_tp_classifier.jpg" width=370> 
       </div>
      <div id="two">
<i><b> Word Spotting in Scene Images based on Character Recognition </b></i>
<p>   In this work we address the challenge of spotting text in scene images without restricting the words to a fixed lexicon or dictionary. Words which are typically out of dictionary include, for instance, cases where exclamation or other punctuation marks are present in words, telephone numbers, URLs, dates, etc.
To this end, we train a Fully Convolutional Network to produce heatmaps of all the character classes. Then, we employ the Text Proposals approach and, via a rectangle classifier, detect the most likely rectangle for each query word based on the character attribute maps. The key advantage of the proposed method is that it allows unconstrained out-of-dictionary word spotting independent from any dictionary or lexicon. We evaluate the proposed method on ICDAR2015 and show that it is capable of identifying and recognizing query words in natural scene images.</p>
<li class="citation">
Bazazian, D., Karatzas, D. and Bagdanov, A., Word Spotting in Scene Images based on Character Recognition, CVPR workshops 2018. [<a class="code" target="_blank" href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w36/Bazazian_Word_Spotting_in_CVPR_2018_paper.pdf">PDF</a>][<a class="code" target="_blank" href="https://github.com/denabazazian/wordSpotting_characterRecognition">Code</a>]
<!-- [<a class="code" target="_blank" href="">PDF</a>] -->
      </div>
</div>
<br>



    <div class="wrapper">
      <div id="one">
      <img src="img/timeSuppression.jpg" width=320> 
       </div>
      <div id="two">
<i><b>FAST: Facilitated and Accurate Scene TextProposal through FCN Guided Pruning</b></i>
<p> Class-specific text proposal algorithms can efficiently reduce the search space for possible text object locations in an image. In this paper we combine the Text Proposals algorithm with Fully Convolutional Networks to efficiently reduce the number of proposals while maintaining the same recall level and thus gaining a significant speed up. Our experiments demonstrate that such text proposal approaches yield significantly higher recall rates than state-of-the-art text localization techniques, while also producing better-quality localizations. Our results on the ICDAR 2015 Robust Reading Competition (Challenge 4) and the COCO-text datasets show that, when combined with strong word classifiers, this recall margin leads to state-of-the-art results in end-to-end scene text recognition.</p>
<li class="citation">
Bazazian, D., Gomez, R., Gomez, L., Nicolaou, A., Karatzas, D., and Bagdanov, A., FAST: Facilitated and Accurate Scene Text Proposals through FCN  Guided  Pruning, Pattern Recognition​ ​ Letter​ ​ (PRL)​ ​ Journal,​ ​ 2017. [<a class="code" target="_blank" href="http://www.sciencedirect.com/science/article/pii/S0167865517302982">PDF</a>][<a class="code" target="_blank" href="https://github.com/denabazazian/FAST_TextProposal">Code</a>]
      </div>
</div>
<br>


    <div class="wrapper">
      <div id="one">
         <img src="img/crops.png" width=320> 
       </div>
      <div id="two">
<i><b>Reading Text in the Wild from Compressed Images </b></i>
<p> Reading text in the wild is gaining attention in the computer vision community. Images captured in the wild are
almost always compressed to varying degrees, depending on application context, and this compression introduces artifacts that distort image content into the captured images. In this paper we investigate the impact these compression
artifacts have on text localization and recognition in the wild. We also propose a deep Convolutional Neural Network (CNN) that can eliminate text-specific compression artifacts and which leads to an improvement in text recognition.  Experimental results on the ICDAR-Challenge4 dataset demonstrate that compression artifacts have a significant impact on text localization and recognition and that our approach yields an improvement in both – especially at high compression rates.</p>
<li class="citation">
Galteri, L., Bazazian, D., Seidenari, L., Bertini, M., Bagdanov, A., Nicolaou, A., Karatzas, D., Reading Text in the Wild from Compressed Images, Egocentric Perception, Interaction and Computing (EPIC) workshop at ICCV2017. [<a class="code" target="_blank" href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w34/degrees_leonardo.galteriunifi.it_dbazaziancvc.uab.es_lorenzo.seidenariunifi.it_ICCV_2017_paper.pdf">PDF</a>][<a class="code" target="_blank" href="https://github.com/denabazazian/ReadingText_CroppedWord">Code</a>]
      </div>
</div>
<br>


    <div class="wrapper">
      <div id="one">
<br> <br> <img src="img/fig_tp_hm_61.jpg" width=350> 
       </div>
      <div id="two">
<i><b>Improving Text Proposals for Scene Images with Fully Convolutional Networks </b></i>
<p> Text  Proposals  have  emerged  as  a  class-dependent version  of  object  proposals  –  efficient  approaches  to  reduce the  search  space  of  possible  text  object  locations  in  an  image. Combined  with  strong  word  classifiers,  text  proposals  currently yield   top   state   of   the   art   results   in   end-to-end   scene   text
recognition.  In  this  paper  we  propose  an  improvement  over  the original Text Proposals algorithm of [1], combining it with Fully Convolutional  Networks  to  improve  the  ranking  of  proposals. Results  on  the  ICDAR  RRC  and  the  COCO-text  datasets  show superior  performance  over  current  state-of-the-art.</p>
<li class="citation">
Bazazian, D., Gomez, R., Gomez, L., Nicolaou, A., Karatzas, D., and Bagdanov, A.,  Improving Text Proposals for Scene Images with Fully Convolutional Networks ,  ICPR-DLPR16 workshop of Deep learning for pattern recognition at ICPR2016.[<a class="code" target="_blank" href="https://arxiv.org/pdf/1702.05089.pdf">PDF</a>][<a class="code" target="_blank" href="https://github.com/denabazazian/pixelWise_textDetector">Code</a>]
      </div>
</div>
<br>


    <div class="wrapper">
      <div id="one">
     <br> <img src="img/DOST.png" width=370> 
       </div>
      <div id="two">
<i><b>ICDAR2017 Robust Reading Challenge on Omnidirectional Video </b></i>
<p> This challenge focuses on scene text localization and recognition on the Downtown Osaka Scene Text (DOST) dataset. Five tasks will be opened within this Challenge: Text Localisation in Videos, Text Localisation in Still Images, Cropped Word Recognition, End-to-End Recognition in Videos, and End-toEnd Recognition in Still Images. The DOST dataset preserves scene texts observed in the real environment as they were. The dataset contains videos (sequential images) captured in shopping streets in downtown Osaka with an omnidirectional camera. Use of the omnidirectional camera contributes to excluding user’s intention in capturing images. Sequential images contained in the dataset contribute to encouraging developing a new kind of text detection and recognition techniques that utilize temporal information. Another important feature of DOST dataset is that it contains non-Latin text. Since the images were captured in Japan, a lot of Japanese text is contained while it also contains adequate amount of Latin text. Because of these features of the dataset, we can say that the DOST dataset preserved scene texts in the wild.</p>
<li class="citation">
Iwamura, M., Morimoto, N., Tainaka, K., Bazazian, D., Gomez, L., Karatzas, D., ICDAR2017 Robust Reading Challenge on Omnidirectional Video, International Conference on Document analysis and Application (ICDAR), 2017.[<a class="code" target="_blank" href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8270167">PDF</a>][<a class="code" target="_blank" href="http://rrc.cvc.uab.es/?ch=7&com=introduction">website</a>]
      </div>
</div>
<br>

    <div class="wrapper">
      <div id="one">
   <img src="img/segmentationBased_edgeExtraction.png" width=350> 
       </div>
      <div id="two">
<i><b>Segmentation-based Multi-Scale Edge Extraction to Measure the Persistence of Features in Unorganized Point Clouds </b></i>
<p> Edge extraction has attracted a lot of attention in computer vision.  The accuracy of extracting edges in point clouds can be a significant asset for a variety of engineering scenarios. To address these issues, we propose a segmentation-based multi-scale edge extraction technique. In this approach, different regions of a point cloud are segmented by a global analysis according to the geodesic distance.  Afterwards, a multi-scale operator is defined according to local neighborhoods. Thereupon, by applying this operator at multiple scales of the point cloud, the persistence of features is determined.  We illustrate the proposed method by computing a feature weight that measures the likelihood of a point to be an edge, then detects the edge points based on that value at both global and local scales.  Moreover, we evaluate quantitatively and qualitatively our method.  Experimental results show that the proposed approach achieves a superior accuracy.  Furthermore, we demonstrate the robustness of our approach in noisier real-world datasets.</p>
<li class="citation">
Bazazian, D., Casas, J.R., Ruiz-Hidalgo, J., “Segmentation-based Multi-Scale Edge Extraction to Measure the Persistence of Features in Unorganized Point Clouds”, VISAPP 2017- International Conference on Computer Vision Theory and Applications.[<a class="code" target="_blank" href="http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0006092503170325">PDF</a>]
      </div>
</div>
<br>


    <div class="wrapper">
      <div id="one">
    <br> <img src="img/RealImages31.png" width=370 height=180> 
       </div>
      <div id="two">
<i><b>Fast and Robust Edge Extraction in unorganized Point Clouds </b></i>
<p> Edges   provide   important   visual   information   in scene  surfaces.  The  need  for  fast  and  robust  feature  extraction from 3D data is nowadays fostered by the widespread availability of  cheap  commercial  depth  sensors  and  multi-camera  setups. This   article   investigates   the   challenge   of   detecting   edges   in surfaces  represented  by  unorganized  point  clouds.  Generally, edge  recognition  requires  the  extraction  of  geometric  features
such as normal vectors and curvatures. Since the normals alone do  not  provide  enough  information  about  the  geometry  of  the
cloud,  further  analysis  of  extracted  normals  is  needed  for  edge extraction, such as a clustering method. Edge extraction through these techniques consists of several steps with parameters which depend  on  the  density  and  the  scale  of  the  point  cloud.  In  this paper we propose a fast and precise method to detect sharp edge features  by  analysing  the  eigenvalues  of  the  covariance  matrix that  are  defined  by  each  point’s  k-nearest  neighbors.  Moreover, we evaluate quantitatively, and qualitatively the proposed methods  for  sharp  edge  extraction  using  several  dihedral  angles  and well known examples of unorganized point clouds. Furthermore, we  demonstrate  the  robustness  of  our  approach  in  the  noisier real-world  datasets</p>
<li class="citation">
Bazazian, D., Casas, J.R., Ruiz-Hidalgo, J.,“Fast and Robust Edge Extraction in unorganized Point Clouds”, DICTA2015 - Digital Image Computing: Techniques and Applications.[<a class="code" target="_blank" href="https://pdfs.semanticscholar.org/ceb4/75ddff146c60c8824ca18605744dd6582660.pdf">PDF</a>] [<a class="code" target="_blank" href="https://github.com/denabazazian/Edge_Extraction">code</a>]
      </div>
</div>
<br>


<a name="Media">
  <h1 class="entry-title">Media</h1>
  <div class="entry-content">
<div class="wrapper">
      <div id="two">
 
<li class="citation">
An interview with the International Association for Pattern Recognition ( IAPR ) about the second IAPR Education Committee Research Scholar. [<a class="code" target="_blank" href="https://iapr.org/docs/newsletter-2019-03.pdf">Link</a> - Pages 7 and 8]  
<li class="citation">
 Fully Convolutional Networks for Text Understanding in Scene Images. CVC, 2018.[<a class="code" target="_blank" href="http://www.cvc.uab.es/?page_id=113&id=299">Video</a>]
<li class="citation">
 The introduction of the fifth Women in Computer Vision Workshop (WiCV) at ECCV 2018.[<a class="code" target="_blank" href="https://www.youtube.com/watch?v=Dk26-cRlzKU&list=PL8KOb02GLRdhKRj6Hvqj2jqkoP2VVUpr8">Video</a>]  
<li class="citation">
 WiCV workshop was featured in the Best of ECCV section of Computer Vision News by RSIPVision. [<a class="code" target="_blank" href="https://www.rsipvision.com/ComputerVisionNews-2018October/20/">Link</a> - Pages 20&mdash;23]  
<li class="citation">
 RSIP article about WiCV at CVPR2018. [<a class="code" target="_blank" href="https://www.rsipvision.com/ComputerVisionNews-2018July/46/">Link</a> - Pages 46&mdash;50]  

</div>
</div>



















